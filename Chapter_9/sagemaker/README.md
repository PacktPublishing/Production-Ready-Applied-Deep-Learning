## Inferencing using SageMaker

In the second half of the chapter, we introduce SageMaker-based deployment. We discuss how to create inference endpoints for TF, PyTorch, and ONNX models. Additionally, the endpoints will be optimized using Amazon SageMaker Neo and El accelerators. The next topic is on how to set up automatic scaling for the inference endpoints running on SageMaker. Finally, we will wrap up the chapter by describing how to host multiple models in a single SageMaker inference endpoint.

* [Inference endpoint for TensorFlow](./tf-inference.ipynb)
* [Inference endpoint for TensorFlow trained model file](./tf_modelfile_deploy.ipynb)
* [Inference endpoint for PyTorch](./pytorch-inference.ipynb)
* [Elastic Inference (EI) accelerator](./ei-inference.ipynb)