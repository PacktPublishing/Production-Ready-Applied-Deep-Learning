## Inferencing using SageMaker

In the second half of the chapter, we introduce SageMaker-based deployment. We discuss how to create inference endpoints for TF, PyTorch, and ONNX models. Additionally, the endpoints will be optimized using Amazon SageMaker Neo and El accelerators. The next topic is on how to set up automatic scaling for the inference endpoints running on SageMaker. Finally, we will wrap up the chapter by describing how to host multiple models in a single SageMaker inference endpoint.

* [Inference endpoint for TensorFlow](./tf-inference.ipynb)
* [Inference endpoint for PyTorch](./pytorch-inference.ipynb)
* [Elastic Inference (EI) accelerator](./ei-inference.ipynb)

# TODO: what is the diff between ei.ipynb and ei-inference.ipynb. Please update the link accordingly. Also, the notebooks are not polished (contains bunch of commented lines, no explanations and errors). Please take a look and make sure they are cleaned up
