{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7be49d",
   "metadata": {},
   "source": [
    "# Network quantization: reducing the number of bits used for model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605497b",
   "metadata": {},
   "source": [
    "A model quantization is the process of remapping a range of numeric values that the model interacts with to a number system that can be represented with fewer bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2590ef5",
   "metadata": {},
   "source": [
    "## Post-training quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9992c6",
   "metadata": {},
   "source": [
    "Let's use a simple dummy model. If you wan to make this exercise more meaningfull feel free to use one of models from\n",
    "```\n",
    "# tf.keras.applications,\n",
    "# for example \n",
    "model = tf.keras.applications.ResNet50(include_top=True, weights='imagenet')\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbb1df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 16:59:49.657295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0-dev20220820\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7fc2abc9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7fc2abc9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 16:59:55.308443: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 0s - loss: 0.7118WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7fc3026e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7fc3026e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "3/3 [==============================] - 1s 64ms/step - loss: 0.6970 - val_loss: 0.7104\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6921 - val_loss: 0.7129\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6917 - val_loss: 0.7200\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6895 - val_loss: 0.7152\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6881 - val_loss: 0.7189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fc2e2a6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# data\n",
    "data = pd.read_csv(\"sample_google_scholar.csv\")\n",
    "data = data.dropna()\n",
    "def convert_first_ten_characters_into_tensor(data):\n",
    "    first_ten_characters = data[:10]\n",
    "    converted = [ord(char)/256 for char in first_ten_characters]\n",
    "    while len(converted) < 10:\n",
    "        converted.append(0.0)\n",
    "    return np.array(converted)\n",
    "converted_affiliation = data['affiliation'].map(convert_first_ten_characters_into_tensor)\n",
    "affiliation = np.vstack(converted_affiliation.values)\n",
    "converted_email = data['email'].str.contains('.edu')\n",
    "labels = converted_email.values\n",
    "# model \n",
    "input_shape = 10\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"layer3\"),\n",
    "    ])\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "# model fit \n",
    "model.fit(affiliation, labels, batch_size=16, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c8274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 128)               1408      \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 64)                8256      \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,729\n",
      "Trainable params: 9,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865587e",
   "metadata": {},
   "source": [
    "We can save this model \n",
    "```python\n",
    "tensorflow_model_path = './tf_model'\n",
    "model.save('./tf_model')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b596d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f7fc3210710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f7fc3210710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f7fc3222680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f7fc3222680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "tensorflow_model_path = './tf_model'\n",
    "model.save('./tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a7a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.hdf5') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f4f74",
   "metadata": {},
   "source": [
    "```\n",
    "and use from_saved_model function from TFLiteConverter   \n",
    "```python\n",
    "tf.lite.TFLiteConverter.from_saved_model(tensorflow_model_path)\n",
    "```\n",
    "or as we already have this model, we can also use from_keras_model from TFLiteConverter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1571506",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f627b5",
   "metadata": {},
   "source": [
    "Finally, we a single line of code we can convert our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb32c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f7fc325a7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f7fc325a7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f7fc3253950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f7fc3253950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: /var/folders/lr/sp74bxw50pz1ylmkv7qtlf_m0000gp/T/tmpxswq5_rj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 16:59:58.290062: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-08-20 16:59:58.290082: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-08-20 16:59:58.290760: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/lr/sp74bxw50pz1ylmkv7qtlf_m0000gp/T/tmpxswq5_rj\n",
      "2022-08-20 16:59:58.292086: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-08-20 16:59:58.292101: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/lr/sp74bxw50pz1ylmkv7qtlf_m0000gp/T/tmpxswq5_rj\n",
      "2022-08-20 16:59:58.295621: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:365] MLIR V1 optimization pass is not enabled\n",
      "2022-08-20 16:59:58.296533: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-08-20 16:59:58.332753: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/lr/sp74bxw50pz1ylmkv7qtlf_m0000gp/T/tmpxswq5_rj\n",
      "2022-08-20 16:59:58.341032: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 50275 microseconds.\n",
      "2022-08-20 16:59:58.358377: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "tfl_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a5ccf",
   "metadata": {},
   "source": [
    "print( dir( tfl_model) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6944ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"tfl_model.tflite\", \"wb\").write(tfl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7da525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model in Mb: 0.14318084716796875\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Original model in Mb:\", os.path.getsize('model.hdf5') / float(2**20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3901f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model in Mb: 0.021404266357421875\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantized model in Mb:\", os.path.getsize('tfl_model.tflite') / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84b120",
   "metadata": {},
   "source": [
    "## Post-training quantization - Full integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e79aa",
   "metadata": {},
   "source": [
    "Full integer quantization where every component for the model inference (inputs, activations, as well as weights) is quantized to lower precision. For this type of quantization, you need to provide a representative dataset to estimate the ranges for the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1248a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rep():\n",
    "    data = affiliation.astype(np.float32)\n",
    "    data = tf.data.Dataset.from_tensor_slices(data).batch(1)\n",
    "    for i in data.take(BATCH_SIZE):\n",
    "        yield [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a47a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./tf_model/')\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter.representative_dataset = gen_rep\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914d9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 16:59:58.660070: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-08-20 16:59:58.660085: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-08-20 16:59:58.660191: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./tf_model/\n",
      "2022-08-20 16:59:58.661288: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-08-20 16:59:58.661299: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./tf_model/\n",
      "2022-08-20 16:59:58.669294: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-08-20 16:59:58.704235: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./tf_model/\n",
      "2022-08-20 16:59:58.712540: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 52350 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba48e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"tflite_quant_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54b9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model in Mb: 0.14318084716796875\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model in Mb:\", os.path.getsize('model.hdf5') / float(2**20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54671686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model in Mb: 0.021404266357421875\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantized model in Mb:\", os.path.getsize('tfl_model.tflite') / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "806b93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model (Full integer quantization¶) in Mb: 0.012176513671875\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantized model (Full integer quantization¶) in Mb:\", os.path.getsize('tflite_quant_model.tflite') / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e7226",
   "metadata": {},
   "source": [
    "## Performing quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa32bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21be7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "input_shape = 10\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"layer3\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ebf696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeAnnotate.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_annotate.QuantizeAnnotate object at 0x7f7fc4699b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeAnnotate.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_annotate.QuantizeAnnotate object at 0x7f7fc4699b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method QuantizeAnnotate.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_annotate.QuantizeAnnotate object at 0x7f7fc4699b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeLayer.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_layer.QuantizeLayer object at 0x7f7fc46c0290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeLayer.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_layer.QuantizeLayer object at 0x7f7fc46c0290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method QuantizeLayer.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_layer.QuantizeLayer object at 0x7f7fc46c0290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeWrapper.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 object at 0x7f7fc469f210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method QuantizeWrapper.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 object at 0x7f7fc469f210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method QuantizeWrapper.call of <tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper.QuantizeWrapperV2 object at 0x7f7fc469f210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "q_aware_model = tfmot.quantization.keras.quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc86dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model.compile(\n",
    "              optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "264cf380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7fc3f8c440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7fc3f8c440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f7fc3f8c440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/3 [=========>....................] - ETA: 1s - loss: 0.6987 - accuracy: 0.5000WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7fc389e560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7fc389e560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f7fc389e560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "3/3 [==============================] - 1s 116ms/step - loss: 0.6940 - accuracy: 0.5208 - val_loss: 0.7283 - val_accuracy: 0.2500\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6951 - accuracy: 0.5208 - val_loss: 0.7351 - val_accuracy: 0.2500\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6886 - accuracy: 0.5208 - val_loss: 0.7303 - val_accuracy: 0.2500\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6918 - accuracy: 0.5208 - val_loss: 0.7309 - val_accuracy: 0.2500\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6892 - accuracy: 0.5208 - val_loss: 0.7222 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fc37b6f10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(affiliation, labels, batch_size=16, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48850ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 10)               3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_layer1 (QuantizeWrapp  (None, 128)              1413      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_layer2 (QuantizeWrapp  (None, 64)               8261      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_layer3 (QuantizeWrapp  (None, 1)                70        \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,747\n",
      "Trainable params: 9,729\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e563a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BookML",
   "language": "python",
   "name": "bookml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
