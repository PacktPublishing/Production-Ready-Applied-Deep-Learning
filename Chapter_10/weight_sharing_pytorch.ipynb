{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019680fa",
   "metadata": {},
   "source": [
    "# Weight Sharing in PyTorch\n",
    "\n",
    "Unfortunately, PyTorch does not support weight sharing. Instead, we will provide a high-level description of a possible implementation. In this example, we will provide a custom implementation of weight sharing for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23fc4145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4815b25fa8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "torch.manual_seed(0)  # set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b7103",
   "metadata": {},
   "source": [
    "### Model with weight clustering support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568f11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with few linear layers \n",
    "class SampleLinearModel(torch.nn.Module): \n",
    "\n",
    "    def __init__(self): \n",
    "        super(SampleLinearModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(10, 10)\n",
    "        \n",
    "    def cluster_weights(self, num_cluster): \n",
    "\n",
    "        # cluster weights of the layer          \n",
    "        km = KMeans(\n",
    "            n_clusters=num_cluster, init='random',\n",
    "            n_init=10, max_iter=300, \n",
    "            tol=1e-04, random_state=0\n",
    "        )\n",
    "        \n",
    "        # construct a mapping from a cluster index to a centroid value and store at self.weights_mapping \n",
    "        weights = model.linear.weight.reshape([-1, 1]).detach().numpy()\n",
    "        self.weights_cluster = km.fit_predict(weights)\n",
    "\n",
    "        # find cluster index for each weight value and store at self.weights_cluster \n",
    "        self.weights_mapping = km.cluster_centers_\n",
    "\n",
    "        # drop the original weights to reduce the model size \n",
    "        self.linear.weight = None\n",
    "\n",
    "    def forward(self, x): \n",
    "        if self.training:\n",
    "            x = self.linear(x)\n",
    "        else: # in eval mode\n",
    "            # update weights of the self.layer by reassigning each value based on \n",
    "            # self.weights_cluster and self.weights_mapping \n",
    "            self.linear.weight = torch.nn.Parameter(torch.Tensor(self.weights_mapping[self.weights_cluster]).reshape(10,10))\n",
    "            x = self.linear(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b4acb",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19137088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.num_samples = 100\n",
    "        self.data = torch.rand([self.num_samples, 10])\n",
    "        self.label = torch.rand([self.num_samples, 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset()\n",
    "training_data_loader = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffdbe6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/.virtualenvs/nova/lib/python3.6/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model = SampleLinearModel() \n",
    "\n",
    "# train the model\n",
    "model.train()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "for data, label in training_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = mse_loss(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a82a8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0296, -0.1846, -0.0341, -0.0587, -0.0704,  0.2134, -0.1665,  0.4920,\n",
      "         0.1367,  0.0739], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "original_output = model(train_dataset[0][0])\n",
    "print(original_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957354ee",
   "metadata": {},
   "source": [
    "### check the original model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "009917a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# save the model and check the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2501f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  original model  \t Size (KB): 1.511\n"
     ]
    }
   ],
   "source": [
    "clustered_model = SampleLinearModel() \n",
    "original_model_size=print_size_of_model(model,\"original model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71e4b3",
   "metadata": {},
   "source": [
    "### Apply weight clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb0673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.cluster_weights(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1164e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weights_mapping: \n",
      " [[-0.20028913]\n",
      " [-0.06823479]\n",
      " [ 0.17053993]\n",
      " [ 0.02950809]\n",
      " [ 0.28837958]]\n",
      "linear.weights_cluster: \n",
      " [3 1 4 4 0 2 0 0 2 3 1 0 1 2 0 0 2 0 0 1 3 0 0 3 0 3 1 0 2 0 0 4 2 4 1 1 3\n",
      " 3 4 0 3 2 1 1 2 0 2 0 2 0 4 0 0 3 4 0 1 4 3 2 0 1 1 4 2 0 2 3 1 1 1 4 0 0\n",
      " 1 3 2 3 1 0 3 1 4 0 0 0 3 4 4 0 3 2 2 4 3 0 4 1 2 1]\n",
      "linear.weight: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"linear.weights_mapping: \\n\", model.weights_mapping)\n",
    "print(\"linear.weights_cluster: \\n\", model.weights_cluster)\n",
    "print(\"linear.weight: \\n\", model.linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0817, -0.1601, -0.0350, -0.0173, -0.1515,  0.2588, -0.2154,  0.4650,\n",
      "         0.1360,  0.0468], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clustered_output = model(train_dataset[0][0])\n",
    "print(clustered_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e520290",
   "metadata": {},
   "source": [
    "### compare the difference in model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98cb4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  clustered model  \t Size (KB): 0.811\n",
      "1.86 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "clustered_model_size=print_size_of_model(model,\"clustered model\")\n",
    "print(\"{0:.2f} times smaller\".format(original_model_size/clustered_model_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
