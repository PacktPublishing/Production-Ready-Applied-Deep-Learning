{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381f02e2",
   "metadata": {},
   "source": [
    "# Network Pruning in PyTorch\n",
    "\n",
    "PyTorch supports post-training network pruning through the module called torch.nn.utils.prune. Given a trained network, pruning can be achieved by passing the model to the global_unstructured function. Once the model is pruned, a binary mask is attached which represents the set of parameters that are pruned. The mask is applied on the target parameter prior to forward operation, eliminating the unnecessary computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed8db2",
   "metadata": {},
   "source": [
    "### Create a sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c57b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)  # set the seed for reproducibility\n",
    "\n",
    "class SampleLinearModel(nn.Module): \n",
    "\n",
    "    def __init__(self): \n",
    "        super(SampleLinearModel, self).__init__() \n",
    "        self.linear1 = nn.Linear(10, 10) \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878276e",
   "metadata": {},
   "source": [
    "### Original model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab98674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleLinearModel(\n",
      "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "original_model = SampleLinearModel()\n",
    "print(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb90e57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in original_model.named_parameters():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd67fb",
   "metadata": {},
   "source": [
    "### Create pruned model\n",
    "\n",
    "In this example, we are pruning the lowest 50% of the weights based on L1-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632243c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleLinearModel(\n",
      "  (linear1): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pruned_model = SampleLinearModel()\n",
    "print(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71011561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "linear1.bias\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in pruned_model.named_parameters():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b7a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "parameters_to_prune = ( \n",
    "    (pruned_model.linear1, 'weight'),\n",
    ") \n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune, \n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2fe04",
   "metadata": {},
   "source": [
    "prunned module will now have a mask and pre hooks for skipping the masked weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e21345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0., 1., 0., 1., 0., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.linear1.weight_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb0cf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.CustomFromMask at 0x7f5e5877a320>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.linear1._forward_pre_hooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
