{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea10f7",
   "metadata": {},
   "source": [
    "# Network Quantization in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7d5d8",
   "metadata": {},
   "source": [
    "## Performing post-training quantization in PyTorch \n",
    "\n",
    "In the case of PyTorch, there are two different post-training quantization methods: dynamic quantization and static quantization. They differ by when the quantization occurs and have different advantages and disadvantages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29aa1f",
   "metadata": {},
   "source": [
    "### Dynamic quantization: quantizing the model at runtime \n",
    "First, we will look at dynamic quantization, the simplest form of quantization available in PyTorch. This type of algorithm applies the quantization on weights ahead of time while quantization on activations occurs dynamically during inference. Therefore, dynamic quantization is often used for situations where the model execution is mainly throttled by loading weights while computing matrix multiplication is not an issue. This type of quantization is often used for LSTM or Transformer networks.\n",
    "\n",
    "Please note that the code we are providing here is based on the official tutorial: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3b624",
   "metadata": {},
   "source": [
    "### Create a sample LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "563a0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)  # set the seed for reproducibility\n",
    "\n",
    "class SampleLSTM(nn.Module):\n",
    "  \"\"\"Sample lstm model\"\"\"\n",
    "\n",
    "  def __init__(self,in_dim,out_dim,depth):\n",
    "     super(SampleLSTM,self).__init__()\n",
    "     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
    "\n",
    "  def forward(self,inputs,hidden):\n",
    "     out,hidden = self.lstm(inputs,hidden)\n",
    "     return out, hidden\n",
    "\n",
    "\n",
    "#shape parameters\n",
    "model_dimension=20\n",
    "sequence_length=10\n",
    "batch_size=1\n",
    "lstm_depth=1\n",
    "\n",
    "# random data for input\n",
    "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
    "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201f6d3",
   "metadata": {},
   "source": [
    "### Apply quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d95fea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "SampleLSTM(\n",
      "  (lstm): LSTM(20, 20)\n",
      ")\n",
      "\n",
      "Quantized model:\n",
      "SampleLSTM(\n",
      "  (lstm): DynamicQuantizedLSTM(20, 20)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# here is our floating point instance\n",
    "original_lstm = SampleLSTM(model_dimension, model_dimension, lstm_depth)\n",
    "\n",
    "# apply quantization on the model\n",
    "quantized_lstm = torch.quantization.quantize_dynamic(\n",
    "    original_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# show the changes that were made\n",
    "print('Original model:')\n",
    "print(original_lstm)\n",
    "print('')\n",
    "print('Quantized model:')\n",
    "print(quantized_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e4f2a",
   "metadata": {},
   "source": [
    "### Compare model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35030896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# save the model and check the model size\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d465655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 14.879\n",
      "model:  int8  \t Size (KB): 5.791\n",
      "2.57 times smaller\n"
     ]
    }
   ],
   "source": [
    "f=print_size_of_model(original_lstm,\"fp32\")\n",
    "q=print_size_of_model(quantized_lstm,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856a4a7",
   "metadata": {},
   "source": [
    "### Compare inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1077206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point FP32: \n",
      "318 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "Quantized INT8: \n",
      "216 µs ± 1.52 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Floating point FP32: \")\n",
    "%timeit original_lstm.forward(inputs, hidden)\n",
    "\n",
    "print(\"Quantized INT8: \")\n",
    "%timeit quantized_lstm.forward(inputs,hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464dbb2",
   "metadata": {},
   "source": [
    "### Compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "858dbe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute value of output tensor values in the FP32 model is 0.11877 \n",
      "mean absolute value of output tensor values in the INT8 model is 0.11880\n",
      "mean absolute value of the difference between the output tensors is 0.00166 or 1.40 percent\n"
     ]
    }
   ],
   "source": [
    "# run the float model\n",
    "out1, hidden1 = original_lstm(inputs, hidden)\n",
    "mag1 = torch.mean(abs(out1)).item()\n",
    "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
    "\n",
    "# run the quantized model\n",
    "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
    "mag2 = torch.mean(abs(out2)).item()\n",
    "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
    "\n",
    "# compare them\n",
    "mag3 = torch.mean(abs(out1-out2)).item()\n",
    "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efccad",
   "metadata": {},
   "source": [
    "### Static quantization: determining optimal quantization parameters using a representative dataset \n",
    "\n",
    "The other type of quantization is called static quantization. Like full integer quantization of TF, this type of quantization minimizes the model performance degradation by estimating the range of numbers that the model interacts with using a representative dataset.\n",
    "\n",
    "Detailed explanation on static quantization can be found here: https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf68f6",
   "metadata": {},
   "source": [
    "### Create a sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11ebc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with few linear layers \n",
    "class SampleLinearModel(torch.nn.Module): \n",
    "\n",
    "    def __init__(self): \n",
    "        super(SampleLinearModel, self).__init__() \n",
    "        # QuantStub converts the incoming floating point tensors into a quantized tensor \n",
    "        self.quant = torch.quantization.QuantStub() \n",
    "        self.linear1 = torch.nn.Linear(10, 100) \n",
    "        self.linear2 = torch.nn.Linear(100, 100) \n",
    "        self.linear3 = torch.nn.Linear(100, 100) \n",
    "        self.linear4 = torch.nn.Linear(100, 100) \n",
    "        self.linear5 = torch.nn.Linear(100, 1) \n",
    "        # DeQuantStub converts the given quantized tensor into a tensor in floating point \n",
    "        self.dequant = torch.quantization.DeQuantStub() \n",
    "\n",
    "    def forward(self, x): \n",
    "        # using QuantStub and DeQuantStub operations, we can indicate the region for quantization \n",
    "        # point to quantized in the quantized model \n",
    "        x = self.quant(x) \n",
    "        x = self.linear1(x) \n",
    "        x = self.linear2(x) \n",
    "        x = self.linear3(x) \n",
    "        x = self.linear4(x) \n",
    "        x = self.linear5(x) \n",
    "        x = self.dequant(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5044502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleLinearModel(\n",
      "  (quant): QuantStub()\n",
      "  (linear1): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (linear5): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for static quantization\n",
    "original_model = SampleLinearModel()\n",
    "print(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffb88e",
   "metadata": {},
   "source": [
    "### Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93db1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCalibrationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.num_samples = 100\n",
    "        self.data = torch.rand([self.num_samples, 10])\n",
    "        self.label = torch.rand([self.num_samples, 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "\n",
    "calibration_dataset = CustomCalibrationDataset()\n",
    "calibration_data_loader = torch.utils.data.DataLoader(calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc88cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleLinearModel(\n",
      "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (linear1): QuantizedLinear(in_features=10, out_features=100, scale=0.017628204077482224, zero_point=57, qscheme=torch.per_channel_affine)\n",
      "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.012031901627779007, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "  (linear3): QuantizedLinear(in_features=100, out_features=100, scale=0.00714643020182848, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "  (linear4): QuantizedLinear(in_features=100, out_features=100, scale=0.005204709246754646, zero_point=58, qscheme=torch.per_channel_affine)\n",
      "  (linear5): QuantizedLinear(in_features=100, out_features=1, scale=0.00029902311507612467, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "original_model.eval()\n",
    "original_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') \n",
    "quantized_model = torch.quantization.prepare(original_model) \n",
    "\n",
    "quantized_model.eval()\n",
    "for data, label in calibration_data_loader:\n",
    "    quantized_model(data)\n",
    "\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42f69f",
   "metadata": {},
   "source": [
    "### Compare model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84850225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 129.031\n",
      "model:  int8  \t Size (KB): 48.261\n",
      "2.67 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "f=print_size_of_model(original_model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a587d9d",
   "metadata": {},
   "source": [
    "## Quantization aware training in PyTorch \n",
    "\n",
    "QAT in PyTorch goes through the similar process. Throughout training, the necessary calculations are achieved in floating point. However, the intermediate values are clamped and rounded to simulate the effect of quantization. The complete details are available at https://pytorch.org/docs/stable/quantization.html. Let’s look at how to set up a QAT for a PyTorch model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939381d",
   "metadata": {},
   "source": [
    "### Create a sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cad5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = SampleLinearModel()\n",
    "\n",
    "training_dataset = CustomCalibrationDataset()\n",
    "training_data_loader = torch.utils.data.DataLoader(calibration_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f669f4",
   "metadata": {},
   "source": [
    "### Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb9b3611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleLinearModel(\n",
      "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (linear1): QuantizedLinear(in_features=10, out_features=100, scale=0.020053371787071228, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.011381133459508419, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "  (linear3): QuantizedLinear(in_features=100, out_features=100, scale=0.007688559591770172, zero_point=57, qscheme=torch.per_channel_affine)\n",
      "  (linear4): QuantizedLinear(in_features=100, out_features=100, scale=0.005497108679264784, zero_point=56, qscheme=torch.per_channel_affine)\n",
      "  (linear5): QuantizedLinear(in_features=100, out_features=1, scale=0.001096641062758863, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "original_model.train()\n",
    "original_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') \n",
    "quantized_model = torch.quantization.prepare_qat(original_model) \n",
    "\n",
    "# train the model\n",
    "quantized_model.train()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(original_model.parameters(), lr=0.001, momentum=0.9)\n",
    "for data, label in training_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    pred = quantized_model(data)\n",
    "    loss = mse_loss(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "quantized_model.eval()\n",
    "torch.quantization.convert(quantized_model, inplace=True)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03b081",
   "metadata": {},
   "source": [
    "### Compare model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0479aadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 129.031\n",
      "model:  int8  \t Size (KB): 48.261\n",
      "2.67 times smaller\n"
     ]
    }
   ],
   "source": [
    "# compare the sizes\n",
    "f=print_size_of_model(original_model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
