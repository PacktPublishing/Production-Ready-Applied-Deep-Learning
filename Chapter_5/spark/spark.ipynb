{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook PySpark Introduction",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### In this notebook we will:\n",
        "- Use *Google Scholar* data set in CSV file format with fields research_interest, author_name, email.\n",
        "- Read *research_interest* field from a csv file in *Google Drive* into RDD and then use *flatMap* and *reduceByKey* to count occurance for each of the research_interest with a map function. *flatMap* helps to apply a transformation on a RDD/DataFrame and convert into another RDD/DataFrame. *reduceByKey* helps to merge the values of keys (words) by applying an reducing operator (add) on it. In our example, we apply add reducing operator on word occurance count on each document/row of a DataFrame.\n",
        "- Apply Aggregate function and sort by column of a DataFrame.\n",
        "- Create a new column and fill it with a *User Defined Function (UDF)* applied on a field in spark DataFrame.\n",
        "- Convert a RDD into a DataFrame with or without schema.\n",
        "- Apply filter on Spark DataFrame.\n",
        "- Write spark dataframe as a single CSV to a Google Drive Folder.\n",
        "- Introduce Spark *Join* concept applied on DataFrame."
      ],
      "metadata": {
        "id": "UOkVs5l3van2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### START of PRE-REQUISITE"
      ],
      "metadata": {
        "id": "v7W_A3C752oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Use apt-get to install basic libraries needed to enable pyspark"
      ],
      "metadata": {
        "id": "xJoH0z_g92BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpdsvwzOinoa",
        "outputId": "ce8539fe-9231-4bcd-e8a5-401bc659c5af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 0 B/88.7 kB 0%] [Connecting to cloud.r-pr\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease 88.7 kB/88.7 kB 100%] [Connecting to cloud.r-project.org] [Wait\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [3 InRelease 47.5 kB/88.7 kB 54%] [Connected to cloud.r-project.org (108.157\r0% [2 InRelease gpgv 242 kB] [3 InRelease 47.5 kB/88.7 kB 54%] [Connected to cl\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 242 kB] [3 InRelease 67.8 kB/88.7 kB 76%] [Connected to cl\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connected to cloud.r-projec\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [2 InRelease gpgv 242 kB] [6 InRelease 9,844 B/74.6 kB 13%] [Connected to cl\r0% [2 InRelease gpgv 242 kB] [Connected to cloud.r-project.org (108.157.4.2)] [\r                                                                               \rGet:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease gpgv 242 kB] [Connected to cloud.r-project.org (108.157.4.2)] [\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to ppa.launchpad\r                                                                               \rGet:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,516 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:24 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.0 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Fetched 14.7 MB in 3s (4,826 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sh-OqOgJigt5"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_distributed = False\n",
        "!pip install -q findspark\n",
        "!pip install pytorch_lightning\n",
        "\n",
        "# !pip install elephas\n",
        "# !pip install analytics-zoo\n",
        "# !pip install bigdl"
      ],
      "metadata": {
        "id": "iYqXcHiUjZyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bda35e4f-7ec9-4a40-c2f2-705ca6c741a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.9-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB)\n",
            "\u001b[K     |████████████████████████████████| 396 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f9fb895d64356ff0275427b68a4f2e5c72b92f272067927fbaad41058b67e934\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 multidict-6.0.2 pyDeprecate-0.3.1 pytorch-lightning-1.5.9 setuptools-59.5.0 torchmetrics-0.7.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "d6buCAqJjshJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "kjZEmPP5jwXv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "FpRSRQoAj2fc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mount Google Drive"
      ],
      "metadata": {
        "id": "f8-pVufG998C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive to the linux running this Colab application\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# list few directories (check access)\n",
        "! ls -ltr /content/drive/MyDrive/dataset/\n",
        "! ls -ltr /content/drive/MyDrive/data_processing/\n",
        "!ls -ltr /content/drive/MyDrive/data_processing/results/top_10_corona_dist/\n",
        "!ls -ltr /content/drive/MyDrive/ |head -2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsqhLY4pmfZy",
        "outputId": "d4e6fb27-3a87-47e2-fcda-75418fb35548"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 17\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_json\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_html\n",
            "drwx------ 2 root root 4096 Jan  4 21:56  dataset_csv\n",
            "drwx------ 2 root root 4096 Jan  4 21:59  dataset_xml\n",
            "-rw------- 1 root root  143 Jan 18 06:19 'ReadME -> dataset.gdoc'\n",
            "total 8\n",
            "drwx------ 2 root root 4096 Jan  4 21:55 results\n",
            "drwx------ 2 root root 4096 Jan  4 21:55 intermediate\n",
            "total 1\n",
            "-rw------- 1 root root 255 Jan 13 19:16 part-00000-de3a3f7b-bff7-49f8-a6ff-8f4cfa8e6fad-c000.csv\n",
            "-rw------- 1 root root   0 Jan 13 19:16 _SUCCESS\n",
            "total 3096140\n",
            "-rw------- 1 root root       143 Apr  3  2012 api id s (1).gsheet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variable Declarations"
      ],
      "metadata": {
        "id": "ICsaQf5y-BzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data sets\n",
        "path_google_scholar = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-google-scholar/output.csv\"\n",
        "# data sets\n",
        "path_covid = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-covid/cdc-pfizer-covid-19-vaccine-distribution-by-state.csv\"\n",
        "# data set taken from https://github.com/nytimes/covid-19-data/blob/master/us-counties.csv\n",
        "path_covid2 = \"/content/drive/MyDrive/dataset/dataset_csv/dataset-covid-2/us-counties.csv\"\n",
        "# base output path\n",
        "path_out_base_result = \"/content/drive/MyDrive/data_processing/results/\""
      ],
      "metadata": {
        "id": "RYna-pF3lJpv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output file for top 10 corona distribution\n",
        "path_out_avg = path_out_base_result + \"top_10_corona_dist\"\n"
      ],
      "metadata": {
        "id": "QzRvASj1_a5H"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT important Libraries\n",
        "import pyspark.sql.functions as F\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import traceback\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from requests import get\n",
        "import requests\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import torch\n",
        "import matplotlib\n",
        "import pytorch_lightning\n",
        "from __future__ import print_function\n",
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "\n",
        "def pretty_print_pandas(title, df, n):\n",
        "  \"\"\" Pretty print\n",
        "  \"\"\"\n",
        "  print(f\"{title}:\")\n",
        "  print(tabulate(df.head(n), headers=\"keys\", tablefmt=\"psql\" ))"
      ],
      "metadata": {
        "id": "OHNdgXFmN-3J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sample read few data sets available"
      ],
      "metadata": {
        "id": "KnHqLJ3p-I4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ input files"
      ],
      "metadata": {
        "id": "Nr8ATpnUM1E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read google scholar\n",
        "df_gs = spark \\\n",
        "        .read \\\n",
        "        .option(\"header\", True) \\\n",
        "        .csv(path_google_scholar)\n",
        "\n",
        "# read covid \n",
        "df_covid = spark \\\n",
        "        .read \\\n",
        "        .option(\"header\", True) \\\n",
        "        .csv(path_covid)\n",
        "df_covid.show(n=2)\n",
        "# read covid 2\n",
        "df_covid2 = spark \\\n",
        "        .read \\\n",
        "        .option(\"header\", True) \\\n",
        "        .csv(path_covid2)\n",
        "df_covid2.show(n=2)\n",
        "c1 = df_covid.select(F.lower(\"jurisdiction\")).distinct().count()\n",
        "c2 = df_covid2.select(F.lower(\"state\")).distinct().count()\n",
        "print(f\"c1: {c1} c2: {c2}\")\n",
        "# unique states from covid ds 1\n",
        "df_covid.select(F.lower(\"jurisdiction\")).distinct().show(n=10)\n",
        "# unique states from covid ds 2\n",
        "df_covid2.select(\"state\").distinct().show(n=10)"
      ],
      "metadata": {
        "id": "xe1D7rboMvml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b909fb-41e9-4ee3-948f-920cfa7f4eb2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+---------------------+---------------------+\n",
            "|jurisdiction| week_of_allocations|_1st_dose_allocations|_2nd_dose_allocations|\n",
            "+------------+--------------------+---------------------+---------------------+\n",
            "| Connecticut|2021-06-21T00:00:...|                54360|                54360|\n",
            "|       Maine|2021-06-21T00:00:...|                21420|                21420|\n",
            "+------------+--------------------+---------------------+---------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+----------+---------+----------+-----+-----+------+\n",
            "|      date|   county|     state| fips|cases|deaths|\n",
            "+----------+---------+----------+-----+-----+------+\n",
            "|2020-01-21|Snohomish|Washington|53061|    1|     0|\n",
            "|2020-01-22|Snohomish|Washington|53061|    1|     0|\n",
            "+----------+---------+----------+-----+-----+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "c1: 63 c2: 56\n",
            "+-------------------+\n",
            "|lower(jurisdiction)|\n",
            "+-------------------+\n",
            "|      west virginia|\n",
            "|      new hampshire|\n",
            "|    mariana islands|\n",
            "|            alabama|\n",
            "|           new york|\n",
            "|   federal entities|\n",
            "|     american samoa|\n",
            "|     north carolina|\n",
            "|            chicago|\n",
            "|       pennsylvania|\n",
            "+-------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------------------+\n",
            "|               state|\n",
            "+--------------------+\n",
            "|                Utah|\n",
            "|              Hawaii|\n",
            "|           Minnesota|\n",
            "|                Ohio|\n",
            "|Northern Mariana ...|\n",
            "|              Oregon|\n",
            "|            Arkansas|\n",
            "|               Texas|\n",
            "|        North Dakota|\n",
            "|        Pennsylvania|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###### END OF PRE-PREQUISITE"
      ],
      "metadata": {
        "id": "xPZHftCT6Gz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Scholar -> calculate frequency of research interest"
      ],
      "metadata": {
        "id": "nyOYfwW7xCLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE FREQUENCY FOR EACH WORD IN RESEARCH INTEREST\n",
        "%%time\n",
        "from pyspark import SparkContext\n",
        "from operator import add\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "# read into dataframe (df)\n",
        "df_gs = spark.read.option(\"header\", True).csv(path_google_scholar)\n",
        "# research_interest can't be None\n",
        "df_gs_clean = df_gs.filter(\"research_interest != 'None'\")\n",
        "# referring Column Names\n",
        "rdd_ri = df_gs_clean.rdd.map(lambda x: (x[\"research_interest\"]))\n",
        "print(\"\\nSample RDD rows:\")\n",
        "print(rdd_ri.take(5))\n",
        "print(\"\\nSample RDD rows after frequenc count for each words:\")\n",
        "# flatMap() helps to apply transformation\n",
        "rdd_ri_freq = rdd_ri.flatMap(lambda x: [(w.lower(), 1) for w in x.split('##')]).reduceByKey(add)\n",
        "# rdd print with take() function\n",
        "print(rdd_ri_freq.take(5))\n",
        "\n",
        "# approach 1 : convert to df without any schema (no proper col names)\n",
        "df_ri_freq = rdd_ri_freq.toDF() \n",
        "\n",
        "pretty_print_pandas(\"RI freq without schema\", df_ri_freq, 10)\n",
        "\n",
        "# approach 2 : convert to df with schema\n",
        "schema = StructType([StructField(\"ri\", StringType(), False), \n",
        "                     StructField(\"frequency\", IntegerType(), False)\n",
        "])\n",
        "# convert rdd to df with schema\n",
        "df = spark.createDataFrame(rdd_ri_freq, schema)\n",
        "print(\"\\nProposed Schema of DF:\")\n",
        "# print schema (to verify)\n",
        "df.printSchema()\n",
        "print(\"\\nRDD converted to DF with schema:\")\n",
        "# sort\n",
        "df_sort = df.sort(F.col(\"frequency\").desc())\n",
        "df_sort.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asgW9dk08_Ux",
        "outputId": "cf8defe1-c493-41d0-8e5e-4adec437f1ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample RDD rows:\n",
            "['data_mining##anomaly_detection', 'artificial_intelligence##machine_learning##data_mining##graph_mining##security', 'machine_learning##never_ending_learning##lifelong_machine_learning##medical_informatics', 'graph_mining##big_data_analytics##machine_learning', 'network_security##cyber_physical_systems_security##cyber_education_and_workforce_development']\n",
            "\n",
            "Sample RDD rows after frequenc count for each words:\n",
            "[('data_mining', 63), ('anomaly_detection', 5), ('artificial_intelligence', 123), ('machine_learning', 198), ('graph_mining', 5)]\n",
            "RI freq without schema:\n",
            "+---------------------------+-----+\n",
            "| 0                         |   1 |\n",
            "|---------------------------+-----|\n",
            "| data_mining               |  63 |\n",
            "| anomaly_detection         |   5 |\n",
            "| artificial_intelligence   | 123 |\n",
            "| machine_learning          | 198 |\n",
            "| graph_mining              |   5 |\n",
            "| security                  |  25 |\n",
            "| never_ending_learning     |   1 |\n",
            "| lifelong_machine_learning |   1 |\n",
            "| medical_informatics       |   7 |\n",
            "| big_data_analytics        |   5 |\n",
            "+---------------------------+-----+\n",
            "\n",
            "Proposed Schema of DF:\n",
            "root\n",
            " |-- ri: string (nullable = false)\n",
            " |-- frequency: integer (nullable = false)\n",
            "\n",
            "\n",
            "RDD converted to DF with schema:\n",
            "+---------------------------+---------+\n",
            "|ri                         |frequency|\n",
            "+---------------------------+---------+\n",
            "|machine_learning           |198      |\n",
            "|artificial_intelligence    |123      |\n",
            "|software_engineering       |94       |\n",
            "|computer_vision            |81       |\n",
            "|data_mining                |63       |\n",
            "|natural_language_processing|63       |\n",
            "|robotics                   |53       |\n",
            "|human_computer_interaction |52       |\n",
            "|information_retrieval      |42       |\n",
            "|deep_learning              |36       |\n",
            "+---------------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "CPU times: user 121 ms, sys: 9.98 ms, total: 131 ms\n",
            "Wall time: 4.09 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This example takes all the columns in the given google scholar file and process the rdd\n",
        "\n",
        "# rdd\n",
        "rdd = spark.sparkContext.textFile(path_google_scholar)\n",
        "print(type(rdd))\n",
        "counts = rdd.flatMap(lambda x: [(w.lower(), 1) for w in x.split(',')]).reduceByKey(add)\n",
        "print(counts.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSq-H4G4c2Sv",
        "outputId": "76fd83c5-1e20-4599-db32-6e6f86440e25"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "[('author_name', 1), ('email', 1), ('affiliation', 1), ('coauthors_names', 1), ('research_interest', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UDF to create a new field \"is_artificial_intellence\" of boolean type"
      ],
      "metadata": {
        "id": "rNul9-kNgIhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType\n",
        "import traceback\n",
        "\n",
        "lst_ai  = [\"data_science\", \"artificial_intelligence\",\n",
        "           \"machine_learning\"]\n",
        "\n",
        "@F.udf\n",
        "def is_ai(research):\n",
        "    \"\"\" return 1 if research in AI domain else 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "      # split the research interest string with delimiter \"##\"  \n",
        "      lst_research = [w.lower() for w in str(research).split(\"##\")]\n",
        "\n",
        "      for res in lst_research:\n",
        "        # if present in AI domain\n",
        "        if res in lst_ai:\n",
        "          return 1\n",
        "      # not present in AI domain\n",
        "      return 0\n",
        "    except:\n",
        "      return -1\n",
        " \n",
        "# df read \n",
        "df_gs = spark.read.option(\"header\", True).csv(path_google_scholar)\n",
        "# create a new column \"is_artificial_intelligence\"\n",
        "df_gs_new = df_gs.withColumn(\"is_artificial_intelligence\",\\\n",
        "                             is_ai(F.col(\"research_interest\")))\n",
        "# df_gs_new.printSchema()\n",
        "df_gs.show(5, truncate=False)\n",
        "df_gs_new.show(n=20)\n",
        "print(f\"Verify that is_ai should have only two distinct value: 0 & 1\")\n",
        "df_gs_new.select(\"is_artificial_intelligence\").distinct().show(5)\n",
        "# show selective columns for analysis\n",
        "df_gs_new[df_gs_new[\"author_name\"].isin([\"Christa Cody\", \"Gabriel Weimann\", \"\"])]\\\n",
        "    .select(\"author_name\",\"research_interest\",\"is_artificial_intelligence\")\\\n",
        "    .show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OE25zYCgHJV",
        "outputId": "7b5f4986-7ff3-4fde-8b8a-d7c976f45fcc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|author_name           |email             |affiliation                       |coauthors_names                                                                                                                                      |research_interest                                                                           |\n",
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|William Eberle        |tntech.edu        |Tennessee Technological University|null                                                                                                                                                 |data_mining##anomaly_detection                                                              |\n",
            "|Lawrence Holder       |wsu.edu           |Washington State University       |Diane J Cook##William Eberle                                                                                                                         |artificial_intelligence##machine_learning##data_mining##graph_mining##security              |\n",
            "|Talbert DA            |tntech.edu        |Tennessee Technological University|null                                                                                                                                                 |machine_learning##never_ending_learning##lifelong_machine_learning##medical_informatics     |\n",
            "|Dr. Sirisha Velampalli|crraoaimscs.res.in|null                              |William Eberle##Lenin Mookiah                                                                                                                        |graph_mining##big_data_analytics##machine_learning                                          |\n",
            "|Ambareen Siraj        |tntech.edu        |Tennessee Tech University         |Rayford Vaughn##William Eberle##Siddharth Kaza##Christa Cody##Khaled Rabieh##Mohammad Ashiqur Rahman##Lenin Mookiah##Ferrol Aderholdt##Brandon Malone|network_security##cyber_physical_systems_security##cyber_education_and_workforce_development|\n",
            "+----------------------+------------------+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------------+\n",
            "|         author_name|              email|         affiliation|     coauthors_names|   research_interest|is_artificial_intelligence|\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------------+\n",
            "|      William Eberle|         tntech.edu|Tennessee Technol...|                null|data_mining##anom...|                         0|\n",
            "|     Lawrence Holder|            wsu.edu|Washington State ...|Diane J Cook##Wil...|artificial_intell...|                         1|\n",
            "|          Talbert DA|         tntech.edu|Tennessee Technol...|                null|machine_learning#...|                         1|\n",
            "|Dr. Sirisha Velam...| crraoaimscs.res.in|                null|William Eberle##L...|graph_mining##big...|                         1|\n",
            "|      Ambareen Siraj|         tntech.edu|Tennessee Tech Un...|Rayford Vaughn##W...|network_security#...|                         0|\n",
            "|        Christa Cody|           ncsu.edu|North Carolina St...|                null|artificial_intell...|                         1|\n",
            "|      Siddharth Kaza|         towson.edu|   Towson University|Byron Marshall##J...|                null|                         0|\n",
            "|    Ferrol Aderholdt|         nvidia.com|              NVIDIA|                null|                null|                         0|\n",
            "|       Khaled Rabieh|           shsu.edu|                null|Mohamed M. E. A. ...|information_secur...|                         0|\n",
            "|      Byron Marshall|bus.oregonstate.edu|Oregon State Univ...|Hsinchun Chen##Si...|it_security##know...|                         0|\n",
            "|       Peter Kawalek|        lboro.ac.uk|Loughborough Univ...|                null|information_syste...|                         0|\n",
            "|Fatemeh “Mariam” ...|            uwm.edu|University of Wis...|Gaurav Bansal##Ja...|web_based_it_syst...|                         0|\n",
            "|Mohamed M. E. A. ...|         tntech.edu|Tennessee Tech Un...|                null|security_and_priv...|                         0|\n",
            "|      Josh Dehlinger|         towson.edu|   Towson University|Robyn Lutz##Suran...|software_engineering|                         0|\n",
            "|          Jaeki Song|            ttu.edu|Texas Tech Univer...|                null|                null|                         0|\n",
            "|       Wingyan Chung|            wcu.edu|Western Carolina ...|Hsinchun Chen##Da...|business_analytic...|                         1|\n",
            "|        Ahmed Abbasi|             nd.edu|University of Not...|Hsinchun Chen##Fa...|artificial_intell...|                         1|\n",
            "|     Gabriel Weimann|    com.haifa.ac.il| University of Haifa|Jonathan Cohen##W...|political_communi...|                         0|\n",
            "|       Lenin Mookiah|students.tntech.edu|                null|William Eberle##L...|graph_mining##ano...|                         0|\n",
            "|        Diane J Cook|       eecs.wsu.edu|Washington State ...|Lawrence Holder##...|artificial_intell...|                         1|\n",
            "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Verify that is_ai should have only two distinct value: 0 & 1\n",
            "+--------------------------+\n",
            "|is_artificial_intelligence|\n",
            "+--------------------------+\n",
            "|                         0|\n",
            "|                         1|\n",
            "+--------------------------+\n",
            "\n",
            "+---------------+-----------------------------------------------------------------+--------------------------+\n",
            "|author_name    |research_interest                                                |is_artificial_intelligence|\n",
            "+---------------+-----------------------------------------------------------------+--------------------------+\n",
            "|Christa Cody   |artificial_intelligence##machine_learning##educational_technology|1                         |\n",
            "|Gabriel Weimann|political_communication##terrorism##media_effects                |0                         |\n",
            "+---------------+-----------------------------------------------------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference:\n",
        "[View RDD content](https://stackoverflow.com/questions/25295277/view-rdd-contents-in-python-spark)\n"
      ],
      "metadata": {
        "id": "XbFCm7Id9sCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TOP 10 VACCINE WEEKLY 1ST DOES DISTRIBUTION STATES"
      ],
      "metadata": {
        "id": "hwyicBr9siTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Write as CSV to Google Drive"
      ],
      "metadata": {
        "id": "v-3UnMAMPWcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print sample\n",
        "df_covid.show(n=2, truncate=False)\n",
        "# group by average\n",
        "df_avg_1 = df_covid.groupby(\"jurisdiction\")\\\n",
        "  .agg(F.avg(\"_1st_dose_allocations\")\n",
        "  .alias(\"avg\"))\\\n",
        "  .sort(F.col(\"avg\").desc())\\\n",
        "  .toDF(\"state\", \"avg\")\n",
        "\n",
        "print(\"Top 10 States by 1st dose covid vaccine distribution\")\n",
        "df_avg_1.show(n=10)\n",
        "print(type(df_avg_1))\n",
        "# write top 10 by average corona weekly vaccine states \n",
        "df_avg_1.limit(10) \\\n",
        "        .coalesce(1) \\\n",
        "        .write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"header\", True) \\\n",
        "        .option(\"quoteAll\",True) \\\n",
        "        .csv(path_out_avg)"
      ],
      "metadata": {
        "id": "rnZTzFOPf9vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37556cbe-8195-4dbd-ee0f-eedf212631b0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------------------+---------------------+---------------------+\n",
            "|jurisdiction|week_of_allocations    |_1st_dose_allocations|_2nd_dose_allocations|\n",
            "+------------+-----------------------+---------------------+---------------------+\n",
            "|Connecticut |2021-06-21T00:00:00.000|54360                |54360                |\n",
            "|Maine       |2021-06-21T00:00:00.000|21420                |21420                |\n",
            "+------------+-----------------------+---------------------+---------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "Top 10 States by 1st dose covid vaccine distribution\n",
            "+----------------+----------+\n",
            "|           state|       avg|\n",
            "+----------------+----------+\n",
            "|      California|  561307.5|\n",
            "|           Texas| 384333.75|\n",
            "|         Florida|306883.125|\n",
            "|Federal Entities|  213150.0|\n",
            "|            Ohio| 168761.25|\n",
            "|    Pennsylvania|166415.625|\n",
            "|        New York| 164036.25|\n",
            "|  North Carolina| 147026.25|\n",
            "|         Georgia| 146036.25|\n",
            "|        Illinois| 146036.25|\n",
            "+----------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Rename the Spark dataframe written non-human readable CSV file to human-readable one"
      ],
      "metadata": {
        "id": "yEsUc-QhPYow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "import os\n",
        "\n",
        "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
        "    \"\"\" return list of filenames that ends with suffix\n",
        "    \"\"\"\n",
        "    filenames = listdir(path_to_dir)\n",
        "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
        "\n",
        "# get file name that just wrote (csv file name)\n",
        "path_csv_file_path = path_out_avg + \"/\" + find_csv_filenames(path_out_avg)[0]\n",
        "# output\n",
        "path_new_file = path_out_avg + \"/\" + \"top_10_states.csv\"\n",
        "# old file name and new file name\n",
        "print(f\"path_csv_file_path: {path_csv_file_path} \\n path_new_file: {path_new_file}\")\n",
        "# rename file\n",
        "os.rename(path_csv_file_path, path_new_file)"
      ],
      "metadata": {
        "id": "K0dxkJkoFkx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### PANDAS: Calculate Average For Each State"
      ],
      "metadata": {
        "id": "bTyOqdUMO7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PANDAS - Example of reading a csv and writing as a CSV\n",
        "path_new_file = path_out_avg + \"/\" + \"top_10_states.csv\"\n",
        "print(f\"input file -> {path_new_file}\")\n",
        "# read already existing file\n",
        "df_in_state = pd.read_csv(path_new_file)\n",
        "# sample\n",
        "print(df_in_state.head(2))\n",
        "# calculate avg for each state (the input file already have avg)\n",
        "# just example to use pandas to do same operation of average\n",
        "df_in_top10 = df_in_state.groupby(\"state\")[\"avg\"].mean().to_frame(\"avg\").reset_index()\n",
        "# output 2\n",
        "path_new_file_pd = path_out_avg + \"/\" + \"top_10_states_pandas.csv\"\n",
        "print(f\"output to {path_new_file_pd}\")\n",
        "# write pandas df to csv\n",
        "df_in_top10.to_csv(path_new_file_pd)"
      ],
      "metadata": {
        "id": "d6knMVrNNpGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPARK: Average of 1st and 2nd DOSE"
      ],
      "metadata": {
        "id": "G8HU4IUe4Vst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each state, calculate average # 1st dose, average # 2nd dose\n",
        "\n",
        "print(list(df_covid))\n",
        "\n",
        "# calculate average weekly 1st dose vaccine distribution. \n",
        "df_avg = df_covid.groupby(F.lower(\"jurisdiction\").alias(\"state\"))\\\n",
        "  .agg(F.avg(\"_1st_dose_allocations\").alias(\"avg_1\"), \\\n",
        "       F.avg(\"_2nd_dose_allocations\").alias(\"avg_2\"), \\\n",
        "       F.sum(\"_1st_dose_allocations\").alias(\"sum_1\"), \\\n",
        "       F.sum(\"_2nd_dose_allocations\").alias(\"sum_2\")\n",
        "       ) \\\n",
        "  .sort(F.col(\"avg_1\").desc())\n",
        "\n",
        "\n",
        "df_avg.show(15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqox4liGIXUE",
        "outputId": "ad7ecb34-360c-46d3-fb9d-d9c59fc91727"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Column<'jurisdiction'>, Column<'week_of_allocations'>, Column<'_1st_dose_allocations'>, Column<'_2nd_dose_allocations'>]\n",
            "+----------------+----------+----------+---------+---------+\n",
            "|           state|     avg_1|     avg_2|    sum_1|    sum_2|\n",
            "+----------------+----------+----------+---------+---------+\n",
            "|      california|  561307.5|  561307.5|8980920.0|8980920.0|\n",
            "|           texas| 384333.75| 384333.75|6149340.0|6149340.0|\n",
            "|         florida|306883.125|306883.125|4910130.0|4910130.0|\n",
            "|federal entities|  213150.0|  213150.0|3197250.0|3197250.0|\n",
            "|            ohio| 168761.25| 168761.25|2700180.0|2700180.0|\n",
            "|    pennsylvania|166415.625|166415.625|2662650.0|2662650.0|\n",
            "|        new york| 164036.25| 164036.25|2624580.0|2624580.0|\n",
            "|  north carolina| 147026.25| 147026.25|2352420.0|2352420.0|\n",
            "|        illinois| 146036.25| 146036.25|2336580.0|2336580.0|\n",
            "|         georgia| 146036.25| 146036.25|2336580.0|2336580.0|\n",
            "|        michigan|144826.875|144826.875|2317230.0|2317230.0|\n",
            "|      new jersey| 129138.75| 129138.75|2066220.0|2066220.0|\n",
            "|   new york city| 125178.75| 125178.75|2002860.0|2002860.0|\n",
            "|        virginia| 122906.25| 122906.25|1966500.0|1966500.0|\n",
            "|      washington|  108126.0|  108126.0|1621890.0|1621890.0|\n",
            "+----------------+----------+----------+---------+---------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SPARK: Equi join first covid data set and second covid data set"
      ],
      "metadata": {
        "id": "H83QM7PLSy6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "print(df_covid2.show(2))\n",
        "# groupby Sex\n",
        "df_cases = df_covid2 \\\n",
        "          .groupby(F.lower(\"state\").alias(\"state\")) \\\n",
        "          .agg(F.sum(\"deaths\").alias(\"sum_deaths\"), \\\n",
        "              F.sum(\"cases\").alias(\"sum_cases\"))\n",
        "df_cases.show(n=3)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQBTKGETrnUu",
        "outputId": "9eacdf31-d6a2-4cf1-b078-afe054cd865c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-----+-----+------+\n",
            "|      date|   county|     state| fips|cases|deaths|\n",
            "+----------+---------+----------+-----+-----+------+\n",
            "|2020-01-21|Snohomish|Washington|53061|    1|     0|\n",
            "|2020-01-22|Snohomish|Washington|53061|    1|     0|\n",
            "+----------+---------+----------+-----+-----+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "+-------------+----------+------------+\n",
            "|        state|sum_deaths|   sum_cases|\n",
            "+-------------+----------+------------+\n",
            "|west virginia| 1286901.0|  7.631901E7|\n",
            "|new hampshire|  620816.0| 4.3191729E7|\n",
            "|      alabama| 5005646.0|2.68440532E8|\n",
            "+-------------+----------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark INNER JOIN EXAMPLE & LEFT JOIN EXAMPLE"
      ],
      "metadata": {
        "id": "LxTdsrQgS9-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get total # distinct states from covid (moderna) dataset and covid (ny) dataset. \n",
        "c1 = df_avg.select(\"state\").distinct().count()\n",
        "c2 = df_cases.select(\"state\").distinct().count()\n",
        "print(f\"c1: {c1} | c2: {c2}\")\n",
        "# create an alias for each of the DataFrame to be joined\n",
        "df_m = df_avg.alias(\"df_m\")\n",
        "df_ny = df_cases.alias(\"df_ny\")\n",
        "print(\"EQUI JOIN\")\n",
        "# EQUI JOIN / INNER JOIN -> only matched on both side (DataFrame) on column \"state\" \n",
        "df_inner = df_m.join(df_ny, F.col(\"df_m.state\") == F.col(\"df_ny.state\"), 'inner')\n",
        "lst_interest = [\"df_m.state\", \"df_ny.state\", \"df_m.avg_1\", \"df_m.avg_2\", \"df_ny.sum_deaths\", \"df_ny.sum_cases\"]\n",
        "# print all states\n",
        "df_inner.select(*lst_interest).show(n=73, truncate=False)\n",
        "\n",
        "# total distinct states count\n",
        "c_inner = df_inner.select(\"df_m.state\").distinct().count()\n",
        "print(f\"Total # states after inner join: {c_inner}\")\n",
        " \n",
        "print(\"LEFT JOIN\")\n",
        "# LEFT JOIN -> all rows on the left side table, right side table not having matched values will be \"null\"\n",
        "df_left = df_m.join(df_ny, F.col(\"df_m.state\") == F.col(\"df_ny.state\"), 'left')\n",
        "df_left.show(n=100)\n",
        "\n",
        "# total distinct states count\n",
        "c_left = df_left.select(\"df_m.state\").distinct().count()\n",
        "print(f\"Total # states after inner join: {c_left}\")"
      ],
      "metadata": {
        "id": "7jot8bMjzi6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8874e59a-378b-47c0-bcd9-b38456037f1f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1: 63 | c2: 56\n",
            "EQUI JOIN\n",
            "+--------------------+--------------------+----------+----------+-----------+-------------+\n",
            "|state               |state               |avg_1     |avg_2     |sum_deaths |sum_cases    |\n",
            "+--------------------+--------------------+----------+----------+-----------+-------------+\n",
            "|west virginia       |west virginia       |27675.0   |27675.0   |1286901.0  |7.631901E7   |\n",
            "|new hampshire       |new hampshire       |20711.25  |20711.25  |620816.0   |4.3191729E7  |\n",
            "|alabama             |alabama             |70745.625 |70745.625 |5005646.0  |2.68440532E8 |\n",
            "|new york            |new york            |164036.25 |164036.25 |2.777481E7 |9.36502157E8 |\n",
            "|american samoa      |american samoa      |731.25    |0.0       |0.0        |115.0        |\n",
            "|north carolina      |north carolina      |147026.25 |147026.25 |5992027.0  |4.67823807E8 |\n",
            "|pennsylvania        |pennsylvania        |166415.625|166415.625|1.2362565E7|5.23851352E8 |\n",
            "|alaska              |alaska              |15096.0   |14316.0   |185028.0   |3.6250194E7  |\n",
            "|guam                |guam                |1608.75   |0.0       |74270.0    |5454582.0    |\n",
            "|montana             |montana             |16053.75  |16053.75  |737469.0   |5.3651362E7  |\n",
            "|south dakota        |south dakota        |12571.875 |12571.875 |853232.0   |5.6775656E7  |\n",
            "|south carolina      |south carolina      |72652.5   |72652.5   |4553506.0  |2.8070155E8  |\n",
            "|new jersey          |new jersey          |129138.75 |129138.75 |1.3486713E7|4.4360215E8  |\n",
            "|north dakota        |north dakota        |11362.5   |11362.5   |692269.0   |5.1982823E7  |\n",
            "|texas               |texas               |384333.75 |384333.75 |2.3704731E7|1.398629921E9|\n",
            "|maryland            |maryland            |87570.0   |87570.0   |4441095.0  |2.07774655E8 |\n",
            "|iowa                |iowa                |45416.25  |45416.25  |2644923.0  |1.74798855E8 |\n",
            "|new mexico          |new mexico          |31010.625 |31010.625 |1852175.0  |9.4910941E7  |\n",
            "|massachusetts       |massachusetts       |102341.25 |102341.25 |8835479.0  |3.12014191E8 |\n",
            "|puerto rico         |puerto rico         |51170.625 |51170.625 |1104308.0  |7.7971981E7  |\n",
            "|rhode island        |rhode island        |16312.5   |16312.5   |1264020.0  |6.59792E7    |\n",
            "|oklahoma            |oklahoma            |56081.25  |56081.25  |3015105.0  |2.12148503E8 |\n",
            "|oregon              |oregon              |61356.0   |61356.0   |1331456.0  |1.01072488E8 |\n",
            "|utah                |utah                |40213.125 |40213.125 |1057366.0  |1.88613416E8 |\n",
            "|nevada              |nevada              |42226.875 |42226.875 |2598043.0  |1.54227637E8 |\n",
            "|mississippi         |mississippi         |43143.75  |43143.75  |3571304.0  |1.62521264E8 |\n",
            "|arkansas            |arkansas            |43216.875 |43216.875 |2736833.0  |1.68508952E8 |\n",
            "|tennessee           |tennessee           |96733.125 |96733.125 |5518471.0  |4.05277324E8 |\n",
            "|idaho               |idaho               |24324.0   |24324.0   |1049640.0  |9.2779083E7  |\n",
            "|arizona             |arizona             |100215.0  |100215.0  |7860443.0  |4.10459041E8 |\n",
            "|illinois            |illinois            |146036.25 |146036.25 |1.1800406E7|6.30697591E8 |\n",
            "|florida             |florida             |306883.125|306883.125|1.8528764E7|1.151887856E9|\n",
            "|nebraska            |nebraska            |27675.0   |27675.0   |1060895.0  |1.0341101E8  |\n",
            "|louisiana           |louisiana           |67224.375 |67224.375 |5536410.0  |2.48800266E8 |\n",
            "|washington          |washington          |108126.0  |108126.0  |2954854.0  |2.15481752E8 |\n",
            "|delaware            |delaware            |14771.25  |14771.25  |793264.0   |4.8426949E7  |\n",
            "|michigan            |michigan            |144826.875|144826.875|9563370.0  |4.24843783E8 |\n",
            "|virginia            |virginia            |122906.25 |122906.25 |4887024.0  |3.0728525E8  |\n",
            "|wyoming             |wyoming             |9090.0    |9090.0    |350926.0   |3.0141626E7  |\n",
            "|hawaii              |hawaii              |21661.875 |21661.875 |259654.0   |2.1321822E7  |\n",
            "|ohio                |ohio                |168761.25 |168761.25 |8657378.0  |5.0637242E8  |\n",
            "|kentucky            |kentucky            |64732.5   |64732.5   |3091168.0  |2.23568456E8 |\n",
            "|vermont             |vermont             |10299.375 |10299.375 |121595.0   |1.1306687E7  |\n",
            "|maine               |maine               |20711.25  |20711.25  |378442.0   |2.9753325E7  |\n",
            "|wisconsin           |wisconsin           |84380.625 |84380.625 |3485652.0  |3.15711375E8 |\n",
            "|california          |california          |561307.5  |561307.5  |2.6290276E7|1.723687353E9|\n",
            "|district of columbia|district of columbia|11289.375 |11289.375 |560752.0   |2.355389E7   |\n",
            "|minnesota           |minnesota           |79762.5   |79762.5   |3449643.0  |2.72519821E8 |\n",
            "|georgia             |georgia             |146036.25 |146036.25 |9429107.0  |5.26433442E8 |\n",
            "|colorado            |colorado            |80201.25  |80201.25  |3251720.0  |2.45538016E8 |\n",
            "|indiana             |indiana             |94865.625 |94865.625 |6248282.0  |3.47291696E8 |\n",
            "|missouri            |missouri            |88048.125 |88048.125 |4534677.0  |2.94198561E8 |\n",
            "|connecticut         |connecticut         |53443.125 |53443.125 |4179927.0  |1.49056454E8 |\n",
            "|kansas              |kansas              |41310.0   |41310.0   |2200685.0  |1.49569963E8 |\n",
            "+--------------------+--------------------+----------+----------+-----------+-------------+\n",
            "\n",
            "Total # states after inner join: 54\n",
            "LEFT JOIN\n",
            "+--------------------+----------+----------+---------+---------+--------------------+-----------+-------------+\n",
            "|               state|     avg_1|     avg_2|    sum_1|    sum_2|               state| sum_deaths|    sum_cases|\n",
            "+--------------------+----------+----------+---------+---------+--------------------+-----------+-------------+\n",
            "|       west virginia|   27675.0|   27675.0| 442800.0| 442800.0|       west virginia|  1286901.0|   7.631901E7|\n",
            "|       new hampshire|  20711.25|  20711.25| 331380.0| 331380.0|       new hampshire|   620816.0|  4.3191729E7|\n",
            "|     mariana islands|     780.0|       0.0|  11700.0|      0.0|                null|       null|         null|\n",
            "|             alabama| 70745.625| 70745.625|1131930.0|1131930.0|             alabama|  5005646.0| 2.68440532E8|\n",
            "|    federal entities|  213150.0|  213150.0|3197250.0|3197250.0|                null|       null|         null|\n",
            "|            new york| 164036.25| 164036.25|2624580.0|2624580.0|            new york| 2.777481E7| 9.36502157E8|\n",
            "|      american samoa|    731.25|       0.0|  11700.0|      0.0|      american samoa|        0.0|        115.0|\n",
            "|             chicago| 39808.125| 39808.125| 636930.0| 636930.0|                null|       null|         null|\n",
            "|      north carolina| 147026.25| 147026.25|2352420.0|2352420.0|      north carolina|  5992027.0| 4.67823807E8|\n",
            "|        pennsylvania|166415.625|166415.625|2662650.0|2662650.0|        pennsylvania|1.2362565E7| 5.23851352E8|\n",
            "|              alaska|   15096.0|   14316.0| 226440.0| 214740.0|              alaska|   185028.0|  3.6250194E7|\n",
            "|                guam|   1608.75|       0.0|  25740.0|      0.0|                guam|    74270.0|    5454582.0|\n",
            "|             montana|  16053.75|  16053.75| 256860.0| 256860.0|             montana|   737469.0|  5.3651362E7|\n",
            "|        south dakota| 12571.875| 12571.875| 201150.0| 201150.0|        south dakota|   853232.0|  5.6775656E7|\n",
            "|          micronesia|       0.0|       0.0|      0.0|      0.0|                null|       null|         null|\n",
            "|      south carolina|   72652.5|   72652.5|1162440.0|1162440.0|      south carolina|  4553506.0|  2.8070155E8|\n",
            "|          new jersey| 129138.75| 129138.75|2066220.0|2066220.0|          new jersey|1.3486713E7|  4.4360215E8|\n",
            "| u.s. virgin islands|    2565.0|    2565.0|  41040.0|  41040.0|                null|       null|         null|\n",
            "|        north dakota|   11362.5|   11362.5| 181800.0| 181800.0|        north dakota|   692269.0|  5.1982823E7|\n",
            "|               texas| 384333.75| 384333.75|6149340.0|6149340.0|               texas|2.3704731E7|1.398629921E9|\n",
            "|                iowa|  45416.25|  45416.25| 726660.0| 726660.0|                iowa|  2644923.0| 1.74798855E8|\n",
            "|            maryland|   87570.0|   87570.0|1401120.0|1401120.0|            maryland|  4441095.0| 2.07774655E8|\n",
            "|          new mexico| 31010.625| 31010.625| 496170.0| 496170.0|          new mexico|  1852175.0|  9.4910941E7|\n",
            "|       massachusetts| 102341.25| 102341.25|1637460.0|1637460.0|       massachusetts|  8835479.0| 3.12014191E8|\n",
            "|         puerto rico| 51170.625| 51170.625| 818730.0| 818730.0|         puerto rico|  1104308.0|  7.7971981E7|\n",
            "|        rhode island|   16312.5|   16312.5| 261000.0| 261000.0|        rhode island|  1264020.0|    6.59792E7|\n",
            "|            oklahoma|  56081.25|  56081.25| 897300.0| 897300.0|            oklahoma|  3015105.0| 2.12148503E8|\n",
            "|              oregon|   61356.0|   61356.0| 920340.0| 920340.0|              oregon|  1331456.0| 1.01072488E8|\n",
            "|                utah| 40213.125| 40213.125| 643410.0| 643410.0|                utah|  1057366.0| 1.88613416E8|\n",
            "|         mississippi|  43143.75|  43143.75| 690300.0| 690300.0|         mississippi|  3571304.0| 1.62521264E8|\n",
            "|              nevada| 42226.875| 42226.875| 675630.0| 675630.0|              nevada|  2598043.0| 1.54227637E8|\n",
            "|            arkansas| 43216.875| 43216.875| 691470.0| 691470.0|            arkansas|  2736833.0| 1.68508952E8|\n",
            "|           tennessee| 96733.125| 96733.125|1547730.0|1547730.0|           tennessee|  5518471.0| 4.05277324E8|\n",
            "|               idaho|   24324.0|   24324.0| 364860.0| 364860.0|               idaho|  1049640.0|  9.2779083E7|\n",
            "|             arizona|  100215.0|  100215.0|1603440.0|1603440.0|             arizona|  7860443.0| 4.10459041E8|\n",
            "|            illinois| 146036.25| 146036.25|2336580.0|2336580.0|            illinois|1.1800406E7| 6.30697591E8|\n",
            "|             florida|306883.125|306883.125|4910130.0|4910130.0|             florida|1.8528764E7|1.151887856E9|\n",
            "|           louisiana| 67224.375| 67224.375|1075590.0|1075590.0|           louisiana|  5536410.0| 2.48800266E8|\n",
            "|            nebraska|   27675.0|   27675.0| 442800.0| 442800.0|            nebraska|  1060895.0|  1.0341101E8|\n",
            "|          washington|  108126.0|  108126.0|1621890.0|1621890.0|          washington|  2954854.0| 2.15481752E8|\n",
            "|            delaware|  14771.25|  14771.25| 236340.0| 236340.0|            delaware|   793264.0|  4.8426949E7|\n",
            "|            michigan|144826.875|144826.875|2317230.0|2317230.0|            michigan|  9563370.0| 4.24843783E8|\n",
            "|            virginia| 122906.25| 122906.25|1966500.0|1966500.0|            virginia|  4887024.0|  3.0728525E8|\n",
            "|             wyoming|    9090.0|    9090.0| 145440.0| 145440.0|             wyoming|   350926.0|  3.0141626E7|\n",
            "|              hawaii| 21661.875| 21661.875| 346590.0| 346590.0|              hawaii|   259654.0|  2.1321822E7|\n",
            "|                ohio| 168761.25| 168761.25|2700180.0|2700180.0|                ohio|  8657378.0|  5.0637242E8|\n",
            "|       new york city| 125178.75| 125178.75|2002860.0|2002860.0|                null|       null|         null|\n",
            "|            kentucky|   64732.5|   64732.5|1035720.0|1035720.0|            kentucky|  3091168.0| 2.23568456E8|\n",
            "|             vermont| 10299.375| 10299.375| 164790.0| 164790.0|             vermont|   121595.0|  1.1306687E7|\n",
            "|               maine|  20711.25|  20711.25| 331380.0| 331380.0|               maine|   378442.0|  2.9753325E7|\n",
            "|        philadelphia|  23861.25|  23861.25| 381780.0| 381780.0|                null|       null|         null|\n",
            "|           wisconsin| 84380.625| 84380.625|1350090.0|1350090.0|           wisconsin|  3485652.0| 3.15711375E8|\n",
            "|          california|  561307.5|  561307.5|8980920.0|8980920.0|          california|2.6290276E7|1.723687353E9|\n",
            "|district of columbia| 11289.375| 11289.375| 180630.0| 180630.0|district of columbia|   560752.0|   2.355389E7|\n",
            "|           minnesota|   79762.5|   79762.5|1276200.0|1276200.0|           minnesota|  3449643.0| 2.72519821E8|\n",
            "|             georgia| 146036.25| 146036.25|2336580.0|2336580.0|             georgia|  9429107.0| 5.26433442E8|\n",
            "|    marshall islands|       0.0|       0.0|      0.0|      0.0|                null|       null|         null|\n",
            "|            colorado|  80201.25|  80201.25|1283220.0|1283220.0|            colorado|  3251720.0| 2.45538016E8|\n",
            "|             indiana| 94865.625| 94865.625|1517850.0|1517850.0|             indiana|  6248282.0| 3.47291696E8|\n",
            "|            missouri| 88048.125| 88048.125|1408770.0|1408770.0|            missouri|  4534677.0| 2.94198561E8|\n",
            "|               palau|       0.0|       0.0|      0.0|      0.0|                null|       null|         null|\n",
            "|         connecticut| 53443.125| 53443.125| 855090.0| 855090.0|         connecticut|  4179927.0| 1.49056454E8|\n",
            "|              kansas|   41310.0|   41310.0| 660960.0| 660960.0|              kansas|  2200685.0| 1.49569963E8|\n",
            "+--------------------+----------+----------+---------+---------+--------------------+-----------+-------------+\n",
            "\n",
            "Total # states after inner join: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REFERENCES"
      ],
      "metadata": {
        "id": "1qkESE0hIUIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm\n",
        "https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/\n",
        "https://sparkbyexamples.com/spark/print-the-contents-of-rdd-in-spark-pyspark/\n",
        "# pipelineds RDD creation when using map operation\n",
        "https://stackoverflow.com/questions/44355416/need-instance-of-rdd-but-returned-class-pyspark-rdd-pipelinedrdd"
      ],
      "metadata": {
        "id": "yFQ0JaGJf-p8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
